# [Persistent Congestion](http://cseweb.ucsd.edu/~snoeren/papers/congestion-sigcomm18.pdf)

### Paper summary
The paper studies the extent, scope and consumer harm of persistent interdomain congestion by providing measurements without direct access to the links. The system uses multiple vantage points for measurements together with backend servers for aggregation. The VP measurements use the insight that congestion causes buffering and, implicitly, increased latencies, thus employing the TSLP system for probing inter-AS connections. The links of interest are identified by applying heuristics over a raw topology generated by traceroute-like algorithms. Finally, the findings are confirmed via evaluating the correlation of loss, Youtube performance, NDT throughput, and operator feedback. The main contribution of the paper is providing the ability for 3rd parties and regulators to study peering disputes in an open and objective way.

### Key insight(s)
- The paper allows regulators and third-parties to augment existing measurement fabrics with a lightweight method for capturing potential interconnection performance issues.

- TSLP can be used due to the correlation between congestion and increased latency. Congestion at a link is detected by sending ICMP probes that expire at the near and far end of the link.

- Links of interest are detected via building a raw topology via traceroute-like algorithms and heuristics that use prefixes to AS mapping and AS relationships.

- Throughput, Youtube streaming, and loss measurements are used to confirm the findings.

- Congestion inference is done at the back-end server by looking at episodes of elevated latency via statistical methods and exploring autocorrelation to detect recurring connection issues.

### What could be improved?
On one hand, the paper is valuable from the perspective that not much infrastructure is needed in order to measure a large number of links between major internet service providers. The methodology does not require direct access to the interdomain links. Furthermore, I appreciate the fact that the findings are backed up by observations from practice: e.g. Google-Century link big utilization and Comcast case study.

On the other hand, I think that the effect on users is not thoroughly evaluated. In particular, for the YouTube test-case does the high failure rate actually cause rebuffering? The median throughput decreases down to 9.2Mbps, but this is an acceptable download rate even for 480p. I think that we are actually more interested in what happens at the tail of the distribution since these will be the hurt users. The start-up time is thoroughly explained, though.

Also, there is no evaluation on the ratio between the number of evaluated links and VPs. I think this is relevant so that regulators can estimate the cost of using their methodology. For example, is it feasible to measure a big part of all American inter-AS connections using this methodology?

I think that one of the main followup research points of focus should be the actual cause of the congestion. We can argue that much more information can be obtained without direct access to a link: one network operating the link at high utilization might be inferred by looking at traffic before the near-end link; congestion caused due to peering disputes might be estimated by looking at how easily the congestion could be avoided by adding other links; usage of ECMP can be detected via looking at latencies of packets with different checksums.
